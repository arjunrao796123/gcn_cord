{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SfB-O9_SWgkn",
    "outputId": "442e5d51-ff79-4afb-c706-d69d81851096"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /home/arjun/gqp/lib/python3.8/site-packages (1.16.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/arjun/gqp/lib/python3.8/site-packages (from boto3) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/arjun/gqp/lib/python3.8/site-packages (from boto3) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.20.0,>=1.19.1 in /home/arjun/gqp/lib/python3.8/site-packages (from boto3) (1.19.1)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.25.4; python_version != \"3.4\" in /home/arjun/gqp/lib/python3.8/site-packages (from botocore<1.20.0,>=1.19.1->boto3) (1.25.10)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/arjun/gqp/lib/python3.8/site-packages (from botocore<1.20.0,>=1.19.1->boto3) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/arjun/gqp/lib/python3.8/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.20.0,>=1.19.1->boto3) (1.15.0)\n",
      "Processing /home/arjun/.cache/pip/wheels/fc/e6/eb/4c4b250a0d6562161dcb8667e2cb07a5e20b257fcb75e50b04/fuzzysearch-0.7.3-cp38-cp38-linux_x86_64.whl\n",
      "Requirement already satisfied: attrs>=19.3 in /home/arjun/gqp/lib/python3.8/site-packages (from fuzzysearch) (20.2.0)\n",
      "Installing collected packages: fuzzysearch\n",
      "Successfully installed fuzzysearch-0.7.3\n"
     ]
    }
   ],
   "source": [
    "# install boto3 and fuzzysearch if not already installed\n",
    "!pip install boto3\n",
    "!pip install fuzzysearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/arjun/Downloads'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qtlFLbke2Sob"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/arjun/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "import boto3\n",
    "import os\n",
    "import spacy\n",
    "import io\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image as PImage\n",
    "import re\n",
    "from fuzzysearch import find_near_matches\n",
    "import pandas as pd\n",
    "import torch\n",
    "import boto3\n",
    "from IPython.display import Image, display\n",
    "from PIL import Image as PImage, ImageDraw\n",
    "import time\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from scipy import sparse\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import json\n",
    "from scipy import sparse\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import cv2\n",
    "from grapher import ObjectTree, Graph\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from utils import load_data, accuracy\n",
    "from models import GCN\n",
    "import os\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7NIzapmZQq7u",
    "outputId": "d082cfad-f8a3-4a08-dc75-bb76bece50d5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7QwnYHD4Rt9E",
    "outputId": "2636d88a-972f-40ab-85a5-f47bd9a32e52"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hiN2M1FKHJKz"
   },
   "outputs": [],
   "source": [
    "# Curent AWS Region\n",
    "mySession = boto3.session.Session(region_name='us-east-1')\n",
    "awsRegion = mySession.region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "m_VY8XSxHJK6"
   },
   "outputs": [],
   "source": [
    "# Amazon S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Amazon Textract client\n",
    "textract = boto3.client('textract',region_name=awsRegion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_values_in_dict(sample_dict, key, list_of_values):\n",
    "    \"\"\"Append multiple values to a key in the given dictionary\"\"\"\n",
    "    if key not in sample_dict:\n",
    "        sample_dict[key] = list()\n",
    "    sample_dict[key].extend(list_of_values)\n",
    "    return sample_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_test(df_,img_path): \n",
    "    import sys\n",
    "    sys.argv=[''];\n",
    "    del sys\n",
    "\n",
    "    # Training settings\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='Disables CUDA training.')\n",
    "    parser.add_argument('--fastmode', action='store_true', default=True,\n",
    "                        help='Validate during training pass.')\n",
    "    parser.add_argument('--seed', type=int, default=69, help='Random seed.')\n",
    "    parser.add_argument('--epochs', type=int, default=10000,\n",
    "                        help='Number of epochs to train.')\n",
    "    parser.add_argument('--lr', type=float, default=5e-3,\n",
    "                        help='Initial learning rate.')\n",
    "    parser.add_argument('--weight_decay', type=float, default=1e-4,\n",
    "                        help='Weight decay (L2 loss on parameters).')\n",
    "    parser.add_argument('--hidden', type=int, default=256,\n",
    "                        help='Number of hidden units.')\n",
    "    parser.add_argument('--dropout', type=float, default=0.5,\n",
    "                        help='Dropout rate (1 - keep probability).')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.cuda:\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "    img = cv2.cvtColor(img_path, cv2.COLOR_BGR2GRAY)\n",
    "    tree = ObjectTree()\n",
    "    tree.read(df_, img)\n",
    "\n",
    "    graph_dict, text_list = tree.connect(plot=True, export_df=True)\n",
    "\n",
    "    graph = Graph(max_nodes=len(text_list))\n",
    "\n",
    "    adj, features = graph.make_graph_data(graph_dict, text_list)\n",
    "    new_feat=[]\n",
    "    for rr in range(len(features)):\n",
    "        new_feat.append(np.append(features[rr],(img.shape[0],img.shape[1])))\n",
    "    features = np.array(new_feat)\n",
    "    table_features = np.array(df_[['xmin','ymin','xmax','ymax','revised_distances_vert','revised_distances_hori']])\n",
    "    features = np.concatenate((features,table_features),axis=1)\n",
    "    ## adjacency matrix\n",
    "    adj = sparse.csr_matrix(adj)\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "    adj = adj.todense()\n",
    "    ## adjacency matrix to block diag\n",
    "    from scipy.sparse import block_diag\n",
    "\n",
    "    adj_array = []\n",
    "    adj_array.append(adj)\n",
    "    sparse_block_diag = block_diag(adj_array)\n",
    "\n",
    "\n",
    "    values = sparse_block_diag.data\n",
    "    indices = np.vstack((sparse_block_diag.row, sparse_block_diag.col))\n",
    "\n",
    "    i = torch.LongTensor(indices)\n",
    "    v = torch.FloatTensor(values)\n",
    "    shape = sparse_block_diag.shape\n",
    "\n",
    "    sp_adj = torch.sparse.FloatTensor(i, v, torch.Size(shape)) # torch tensor adj matrix\n",
    "    features = torch.FloatTensor(np.array(features))  # feature matrix for image\n",
    "    return sp_adj,features,args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(df_,img_path):\n",
    "    df_merged_labels = pd.read_csv('/home/arjun/same_set_df_merged_labels.csv')\n",
    "    df_merged_labels.iloc[:, 13:] = df_merged_labels.iloc[:, 13:].fillna(0)\n",
    "    labels = df_merged_labels.iloc[:, 13:]\n",
    "    labels2 = torch.LongTensor(np.where(labels)[1])\n",
    "    sp_adj,features,args = spatial_test(df_,img_path)\n",
    "    model = GCN(nfeat=features.shape[1],\n",
    "                nhid=args.hidden,\n",
    "                nclass=labels2.max().item() + 1,\n",
    "                dropout=args.dropout)\n",
    "    model.load_state_dict(torch.load('/home/arjun/same_set_best_72_robert_40_text_int_22.pt'))\n",
    "    #model.load_state_dict(torch.load('/media/arjun/Seagate Expansion Drive/gqp/features/'))\n",
    "\n",
    "    model.eval()\n",
    "    output = model(features, sp_adj)\n",
    "    preds = output.max(1)[1].type_as(labels2)\n",
    "\n",
    "    return preds,labels\n",
    "    #loss_test = F.cross_entropy(output[idx_test], labels[idx_test])\n",
    "\n",
    "    # print(output[idx_test],'output[idx_test]')\n",
    "    # print(labels_df.columns)\n",
    "    #print(labels[idx_test], 'labels[idx_test]')\n",
    "    # writer.add_scalar('Loss/test', loss_test, epoch)\n",
    "\n",
    "    #acc_test = accuracy(output[idx_test], labels[idx_test],flag=False,target_names= df_merged_labels.iloc[:, 13:].columns)\n",
    "    #acc_train = accuracy(output[idx_train], labels[idx_train], flag=False,\n",
    "                        #target_names=df_merged_labels.iloc[:, 13:].columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_spatial(df_,img_path='/home/arjun/PycharmProjects/pygcn/pygcn/test_files/train_img/final_train/receipt_00000.png'):\n",
    "    preds,labels = test(df_,img_path)\n",
    "    prediction = pd.DataFrame([np.array(preds.detach().cpu()),df_['Object']]).T\n",
    "    prediction['col'] = prediction[0]\n",
    "    for i in range(len(prediction)):\n",
    "        prediction['col'][i] = labels.columns[prediction[0][i]]\n",
    "    dict1 = {}\n",
    "    for i in range(len(prediction)):\n",
    "        add_values_in_dict(dict1,prediction['col'][i],[prediction[1][i]])\n",
    "    \n",
    "    return dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "SXPzV9vMue0o"
   },
   "outputs": [],
   "source": [
    "# function to call the spacy model and predict values and labels\n",
    "def spacy_ner(text_in):\n",
    "    # load model from google drive\n",
    "    model_dir = '/home/arjun/Downloads/Spacy Model-CORD/Spacy Model/'\n",
    "    \n",
    "    nlp = spacy.load(model_dir)\n",
    "\n",
    "    # generate predictions\n",
    "    doc = nlp(text_in)\n",
    "\n",
    "    # return predictions\n",
    "    dicts = {ent.label_ : ent.text for ent in doc.ents}\n",
    "    dict1={}\n",
    "    for ent in doc.ents:\n",
    "        add_values_in_dict(dict1,ent.label_,[ent.text])\n",
    "\n",
    "    return dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "YxqKJjhl783s"
   },
   "outputs": [],
   "source": [
    " def call_textract(input):\n",
    "    # reformat image into bytes\n",
    "    try:\n",
    "      imageBytes = bytearray(input.read())\n",
    "    except:\n",
    "      imageBytes = bytearray(input)\n",
    "\n",
    "    # Call Amazon Textract\n",
    "    decoded = cv2.imdecode(np.frombuffer(imageBytes, np.uint8), -1)\n",
    "    #print(decoded.shape)\n",
    "    response = textract.analyze_document(Document={'Bytes': imageBytes},FeatureTypes=['TABLES','FORMS'])\n",
    "    imgWidth, imgHeight,channel = decoded.shape\n",
    "    \n",
    "    columns = ['xmin','ymin', 'xmax','ymax','Object']\n",
    "\n",
    "    df_ = pd.DataFrame( columns=columns)\n",
    "        \n",
    "    # aggregate output into one document\n",
    "    blocks=response['Blocks']\n",
    "    block_map = {}\n",
    "    for block in blocks:\n",
    "        block_id = block['Id']\n",
    "        block_map[block_id] = block\n",
    "\n",
    "    textfile = ''\n",
    "\n",
    "    # Compile detected text\n",
    "    for item in response[\"Blocks\"]:\n",
    "        if item[\"BlockType\"] == 'WORD':\n",
    "            textfile += item[\"Text\"]+\" \"\n",
    "            x_min = imgWidth*item['Geometry']['Polygon'][0]['X']\n",
    "            x_max = imgWidth*item['Geometry']['Polygon'][1]['X']\n",
    "            y_min = imgHeight*item['Geometry']['Polygon'][0]['Y']\n",
    "            y_max = imgHeight*item['Geometry']['Polygon'][2]['Y']\n",
    "            Object = item['Text']\n",
    "            #print(x_min,'\t',y_min,'\t',x_max,'\t',y_max,'\t',item['Text'])\n",
    "\n",
    "            df_ = df_.append({'xmin': x_min,'ymin':y_min,'xmax':x_max,'ymax':y_max,'Object':Object},ignore_index = True)\n",
    "\n",
    "    return textfile,df_,decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "R0tNW4OHle9M"
   },
   "outputs": [],
   "source": [
    "# sharpen subprocess in preprocessing steps\n",
    "def sharpen(image):\n",
    "    kernel = np.array([[-1,-1,-1], \n",
    "                       [-1, 9,-1],\n",
    "                       [-1,-1,-1]])\n",
    "    sharpened = cv2.filter2D(image, -1, kernel) # applying the sharpening kernel to the input image & displaying it.\n",
    "    return sharpened\n",
    "\n",
    "# clahe subprocess in preprocessing steps\n",
    "def clahe(img):\n",
    "    #-----Converting image to LAB Color model----------------------------------- \n",
    "    lab= cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "    #-----Splitting the LAB image to different channels-------------------------\n",
    "    l, a, b = cv2.split(lab)\n",
    "\n",
    "    #-----Applying CLAHE to L-channel-----/--------------------------------------\n",
    "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
    "    cl = clahe.apply(l)\n",
    "\n",
    "    #-----Merge the CLAHE enhanced L-channel with the a and b channel-----------\n",
    "    limg = cv2.merge((cl,a,b))\n",
    "\n",
    "    #-----Converting image from LAB Color model to RGB model--------------------\n",
    "    final = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "    return final\n",
    "\n",
    "# preprocessing method 1\n",
    "def cv_image_07(fullPath):\n",
    "    image = cv2.imread(fullPath, cv2.IMREAD_UNCHANGED);\n",
    "    image = sharpen(image)\n",
    "    image = cv2.GaussianBlur(image,(3,3),cv2.BORDER_DEFAULT)\n",
    "    image = cv2.fastNlMeansDenoisingColored(image,None,10,10,7,21)  \n",
    "    gray = cv2.edgePreservingFilter(image, flags=1, sigma_s=400, sigma_r=0.2)\n",
    "    gray = cv2.edgePreservingFilter(image, flags=1, sigma_s=50, sigma_r=0.002)\n",
    "    alpha = 1.5 # Contrast control (1.0-3.0)\n",
    "    beta = 0 # Brightness control (0-100)\n",
    "    gray = cv2.convertScaleAbs(gray, alpha=alpha, beta=beta)\n",
    "    return gray\n",
    "\n",
    "# preprocessing method 2\n",
    "def cv_image_05(fullPath):\n",
    "    image = cv2.imread(fullPath, cv2.IMREAD_UNCHANGED);\n",
    "    image = clahe(image)\n",
    "    image = cv2.fastNlMeansDenoisingColored(image,None,10,10,7,21)\n",
    "    image = sharpen(image)\n",
    "    gray = cv2.edgePreservingFilter(image, flags=1, sigma_s=400, sigma_r=0.2)\n",
    "    alpha = 1.5 # Contrast control (1.0-3.0)\n",
    "    beta = 0 # Brightness control (0-100)\n",
    "    gray = cv2.convertScaleAbs(gray, alpha=alpha, beta=beta)\n",
    "    return gray\n",
    "\n",
    "# preprocessing method 3\n",
    "def cv_image_74_82(fullPath):\n",
    "    image = cv2.imread(fullPath, cv2.IMREAD_UNCHANGED);\n",
    "    image = clahe(image)\n",
    "    gray = cv2.edgePreservingFilter(image, flags=1, sigma_s=400, sigma_r=0.2)\n",
    "    alpha = 1.5 # Contrast control (1.0-3.0)\n",
    "    beta = 0# Brightness control (0-100)\n",
    "    gray = cv2.convertScaleAbs(gray, alpha=alpha, beta=beta)\n",
    "    return gray\n",
    "\n",
    "# preprocessing method 4\n",
    "def cv_image_27(fullPath):\n",
    "    image = cv2.imread(fullPath, cv2.IMREAD_UNCHANGED);\n",
    "    image = clahe(image)\n",
    "    image = cv2.detailEnhance(image, sigma_s=150, sigma_r=0.15)\n",
    "    image = cv2.fastNlMeansDenoisingColored(image,None,10,10,7,21)\n",
    "    gray = cv2.edgePreservingFilter(image, flags=1, sigma_s=400, sigma_r=0.2)\n",
    "    gray = cv2.fastNlMeansDenoisingColored(gray,None,10,10,7,21)\n",
    "    return gray\n",
    "    \n",
    "# preprocessing method 5\n",
    "def cv_image_11(fullPath):\n",
    "    image = cv2.imread(fullPath, cv2.IMREAD_UNCHANGED);\n",
    "    image = cv2.detailEnhance(image, sigma_s=200, sigma_r=0.05)\n",
    "    gray = cv2.edgePreservingFilter(image, flags=1, sigma_s=400, sigma_r=0.2)\n",
    "    gray = cv2.bitwise_not(gray)\n",
    "    alpha = 1.5 # Contrast control (1.0-3.0)\n",
    "    beta = 0 # Brightness control (0-100)\n",
    "    gray = cv2.convertScaleAbs(gray, alpha=alpha, beta=beta)\n",
    "    return gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "mTu7GcruUwS2"
   },
   "outputs": [],
   "source": [
    "# reformats the image from the cv2 data type into a temp file bytes object that\n",
    "# can be read by textract\n",
    "def reformat(cv_img):\n",
    "    # convert image to PIL format\n",
    "    pil_im = PImage.fromarray(cv_img)\n",
    "    # create temp file object\n",
    "    img_byte_arr = io.BytesIO()\n",
    "    # save image to temp file object as .png\n",
    "    pil_im.save(img_byte_arr, format ='PNG')\n",
    "    # retrieve values from temp image file\n",
    "    img_byte_arr = img_byte_arr.getvalue()\n",
    "    return img_byte_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "KfLP19TFWooH"
   },
   "outputs": [],
   "source": [
    "# master function that gradio calls\n",
    "def textout(input,ppyn,method_option):   \n",
    "    # if user selects no preprocessing\n",
    "    if ppyn == \"No\":\n",
    "        # call textract to return text\n",
    "        nopp, df_,decoded = call_textract(input)\n",
    "        # create label predictions from that text\n",
    "        if method_option == 'Spacy':\n",
    "            nopppred = spacy_ner(nopp)\n",
    "        if method_option == 'Spatial':\n",
    "            nopppred = test_spatial(df_,decoded)\n",
    "        # convert image for display back to user\n",
    "        pil_im = PImage.open(input.name)\n",
    "        # return brand, label predictions and image back to user\n",
    "        return nopppred, pil_im\n",
    "\n",
    "    # if using preprocessing\n",
    "    else:  \n",
    "        # list of preprocessing functions\n",
    "        funcs = [cv_image_07, cv_image_05, cv_image_74_82, cv_image_27, cv_image_11]\n",
    "        # list of outputs from each preprocessing function\n",
    "        lens = [] # length of label predictions\n",
    "        preds = [] # label predictions\n",
    "        imgs = [] # image used\n",
    "\n",
    "        # first run with no preprocessing to get brand and predictions\n",
    "        nopp,df_,decoded = call_textract(input)\n",
    "        if method_option == 'Spatial':\n",
    "            nopppred = test_spatial(df_,decoded)\n",
    "        if method_option == 'Spacy':\n",
    "            nopppred = spacy_ner(nopp)\n",
    "\n",
    "        # append prediction, length of prediction, brand and image to lists\n",
    "        preds.append(nopppred)\n",
    "        lens.append(sum(len(x) for x in nopppred.values()))\n",
    "        imgs.append(PImage.open(input.name))\n",
    "\n",
    "        # iterate through preprocessing functions\n",
    "        for f in funcs:\n",
    "            # apply preprocessing\n",
    "            pp = f(input.name)\n",
    "\n",
    "            # run with preprocessed image to get brand and predictions\n",
    "            pptext,df_,decoded = call_textract(reformat(pp))\n",
    "            if method_option =='Spacy':\n",
    "                pppred = spacy_ner(pptext)\n",
    "            if method_option == 'Spatial':\n",
    "                pppred = test_spatial(df_,decoded)\n",
    "\n",
    "\n",
    "            # append prediction, length of prediction, brand and image to lists\n",
    "            preds.append(pppred)\n",
    "            lens.append(sum(len(x) for x in pppred.values()))\n",
    "            imgs.append(pp)\n",
    "\n",
    "        # calculate which method yields the longest predicted label\n",
    "        biggest_output = lens.index(max(lens))\n",
    "\n",
    "        # return to user the brand, label prediction and image used that yielded \n",
    "        # the longest predicted label\n",
    "        final_pred = preds[biggest_output]\n",
    "        final_img = PImage.fromarray(np.asarray(imgs[biggest_output]))\n",
    "\n",
    "        # return brand, label predictions and image back to user\n",
    "        return final_pred, final_img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R06dbZZaYJDq"
   },
   "source": [
    "Now we'll wrap this function with a Gradio interface."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "id": "fJUJLWQ92g6R",
    "outputId": "0d847f51-1748-4412-9067-1a5ddb395b55"
   },
   "source": [
    "!pip install -q gradio\n",
    "!pip install -I flask==1.1.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 888
    },
    "id": "e200MmBU2aLT",
    "outputId": "d1c576ce-f6b6-483b-f6d8-6ba4bc068da3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally at: http://127.0.0.1:7865/\n",
      "To get a public link for a hosted model, set Share=True\n",
      "Interface loading below...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"500\"\n",
       "            src=\"http://127.0.0.1:7865/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f0017583e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<Flask 'gradio.networking'>, 'http://127.0.0.1:7865/', None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio\n",
    "\n",
    "# image input function\n",
    "image = gradio.inputs.Image(shape=None, source=\"upload\", tool=\"select\", type=\"file\",label = \"Nameplate Image\")\n",
    "# preprocessing yes/no radio button\n",
    "pp_yn = gradio.inputs.Radio(choices=[\"Yes\",\"No\"],label = \"Use Preprocessing on Image?\")\n",
    "method_option = gradio.inputs.Radio(choices=[\"Spatial\",\"Spacy\"],label = \"Which method?\")\n",
    "# create interface that calls the textout function\n",
    "gradio.Interface(textout, # function\n",
    "    [image,pp_yn,method_option], # inputs\n",
    "    [\n",
    "      # outputs\n",
    "      gradio.outputs.KeyValues(label=\"Predicted Values\"),\n",
    "      gradio.outputs.Image(type='pil',label=\"Image used for Predictions\")    \n",
    "    ]).launch(debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CORD Text Extraction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
